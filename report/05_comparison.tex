\section{Comparison}
\label{sec:comparison}

\subsection{Baseline}
The best \ac{WFST} model in the mid-term project used a 4-gram language model for the concept tags and achieved a F1 score of $82.74\%$.
The baseline \ac{CRF} model described in \cref{subsection:words} achieved a F1 score of $81.39\%$.
Both models used only the words as a feature.
\ac{CRF} models, due to the \texttt{CRF++} limitations and the model complexity, can only take into account the current and the previous label for the classification.
In the \ac{WFST} model, this would correspond to a 2-gram language model for the concepts, which performed $79.45\%$ F1 score.
\ac{WFST} models are simpler to build and can thus use more complex language models that can outperform more complex generative methods.

\subsection{Complex Models}
The most advanced \ac{CRF} models trained described in \cref{subsection:additional} achieved a F1 score of $83.57\%$.
The model combines words, POS tags, stems, prefixes and radixes.
This shows how discriminative models can exploit additional information that can not be easily modeled with generative models.
Thanks to this additional information, \ac{CRF} outperformed \ac{WFST} on the concept tagging task.

\begin{table}[h]
	\centering
    \begin{tabular}{ c c c c }
    	\toprule
    		\multicolumn{1}{c}{algorithm} & \multicolumn{1}{c}{prec.} & \multicolumn{1}{c}{rec.} & \multicolumn{1}{c}{F1} \\
    	\midrule
            WFST (4-gram) & 82.44\% & 83.04\% & 82.74\% \\
            CRF (manual) & 86.72\% & 80.66\% & 83.57\% \\
			CRF (genetic) & 87.52\% & 81.03\% & 84.15\% \\
    	\bottomrule
	\end{tabular}
    \caption{Comparison of the performances of the best models for \ac{WFST} and \ac{CRF}.}
	\label{tab:best}
\end{table}

\cref{tab:best} compares the performances of best \ac{WFST} and \ac{CRF} models.
We can notice that the \ac{WFST} model has a significantly higher recall than both \ac{CRF} models.
\ac{WFST} tends to be more conservative in the classification, probably due to the more complex language model (4-grams) than tends to prevent false positives.
On the other hand, \ac{CRF} models exploit multiple features and are able to find more concepts, at the cost of some extra false positive.

\subsection{Training Complexity}
\ac{WFST} are very easy to train, since the training only requires to estimate some probability distributions from the training data.
This can be done by simply compute frequencies and normalizing them to probabilities.
Some effort should be put into the choice of the meta-parameters such as the window size for the n-gram language model and the smoothing method.

In contrast, \ac{CRF} require to manually specify the features to use.
As described in \cref{sec:experiments}, this can be very difficult due to the exponential number of possible combinations.
The problem can be partially solved using optimization techniques such as genetic algorithms.
These methods automatically pick the best features combination, at the cost of some additional computations in the training phase.
In our case, the genetic algorithm managed to improved significantly the model performances, as described in \cref{subsection:genetic}.
