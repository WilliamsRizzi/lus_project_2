\section{Experiments}
\label{sec:experiments}

\subsection{Words}
\label{subsection:words}
The first trial was to train a \ac{CRF} using only the words of the sentence.
The result was used as the baseline for the evaluation of the performances of more complex models.
We experimented using different window sizes, i.e. creating a feature function for the current word and the preceding and following $n$ words, with $n \in [0,6]$.
In addition, we tried to enable the bigram templates of \texttt{CRF++}, i.e. considering also the label of the previous word to classify the current one. 

\begin{table}[h]
	\centering
    \begin{tabular}{ c c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{F1, no bigram} & \multicolumn{1}{c}{F1, bigram} \\
    	\midrule
            \input{table/01_words}
    	\bottomrule
	\end{tabular}
    \caption{Performances of the models using only the words. For each window size, the model is trained with and without the bigram template.}
	\label{tab:words}
\end{table}

\cref{tab:words} summaries the performances of the different models.
In all cases, the bigram template helps to get significantly higher performances.
The best model is the one with bigrams and window size $2$.
The concept of the previous word seems to be very important to better classify the current one (bigram template).
The performances improve up to window size $2$, then degrades for larger ones.
The words very distant from the current one seems to have little or no effect on the semantic of the current one.
Taking them into account makes the model overfit the training data.

\subsection{POS}
\label{subsection:pos}
The next step was to try the other features initially provided, i.e. \ac{POS} and word stems.
Given the results presented in \cref{subsection:words}, we considered only templates using the bigrams.

We started to add \ac{POS} features to the best template in the previous section (windows size of $2$), using again the window approach with $n \in [0,4]$.
The results are shown in \cref{tab:pos}.
As expected, the \ac{POS} features improved the performances of the classifier.
The best model was obtained for $n = 2$.
Similarly to the previous models, a large window seems to overfit the training data.

\begin{table}[h]
	\centering
    \begin{tabular}{ c c c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{prec.} & \multicolumn{1}{c}{rec.} & \multicolumn{1}{c}{F1} \\
    	\midrule
            \input{table/02_pos}
    	\bottomrule
	\end{tabular}
    \caption{Performances of the models using words and POS tags for different window sizes.}
	\label{tab:pos}
\end{table}

\subsection{Stems}
\label{subsection:stems}
We repeated the same procedure for the word stems, using a window size $n \in [0,4]$.
The stem feature helped to obtain a slightly improvement, even though not significant.
A window size of $0$ made the classifier performances slightly better, while all other attempts gave a worsening.
The results are reported in \cref{tab:stems}.

\begin{table}[h]
	\centering
    \begin{tabular}{ c c c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{prec.} & \multicolumn{1}{c}{rec.} & \multicolumn{1}{c}{F1} \\
    	\midrule
            \input{table/03_stems}
    	\bottomrule
	\end{tabular}
    \caption{Performances of the models using words, POS tags and stems for different window sizes.}
	\label{tab:stems}
\end{table}

\subsection{Additional Features}
\label{subsection:additional}
Since many concepts spans on multiple words, we added features for sequences of successive 2 words, \ac{POS} tags and stems.
Then, we tried to combine words and \ac{POS} tags of closed words.
Unfortunately, the performances decreased in both cases.

Since both POS and Stems seemed to be useful for the classifier, we tried combine larger window sizes for words, \ac{POS} and stems with the new addition features.
After some attempts, we were able to increase the performances of the model to F1 of $82.63\%$.

Finally, we added the features generated from the available ones, as described in section \cref{subsection:preprocessing}.
The features ``stem is capitalized'' and ``word is a language'' did not help the classifier at all.
On the other hand, prefixes and suffixes improved performances, getting a F1 score of $83.44\%$

\subsection{Regularization}
We tried different regularization parameters (\texttt{c} option in \texttt{CRF++}) for the best model found so far.
\cref{tab:regularization} show the performances for different \texttt{c} values.
We can notice that the parameter slightly influences the final performances.

\begin{table}[h]
	\centering
    \begin{tabular}{ c c c c }
    	\toprule
    		\multicolumn{1}{c}{c} & \multicolumn{1}{c}{prec.} & \multicolumn{1}{c}{rec.} & \multicolumn{1}{c}{F1} \\
    	\midrule
            \input{table/04_regularization}
    	\bottomrule
	\end{tabular}
    \caption{Performances of the the best \ac{CRF} model, trained using different regularization parameters.}
	\label{tab:regularization}
\end{table}

\subsection{Genetic Algorithm}
\label{subsection:genetic}
In order to select the best feature functions for the concept tagging task, we decided to the features selection as a black-box optimization problem.
We developed a genetic algorithm using the Python \texttt{DEAP}\footnote{\url{https://github.com/DEAP/deap}} library \cite{deap}.
Following the genetic analogy \cite{lion}, we treated a \ac{CRF} template as an individual and a feature function as a gene.
A set of individuals form a population.

\subsubsection{Initialization}
The algorithm is divided in two phases.
The first one consists in the population initialization.
In our case, the initial population is composed by the union of the best scoring \ac{CRF} models with some randomly generated individuals.
To generate a new individual, it is sufficient to randomly select a set of genes.
The bigram options is automatically added to the entire population to guarantee better performances.

\subsection{Evolution}
The second phase is the evolution.
The evolution lasts several epochs.
Each epoch generates a new population starting from the current one, trying to select the best individuals.
After the last iteration, the population contains the best individuals found during the search.
At each iteration, the following steps are performed.

\paragraph{Selection}
The population is evaluated using the fitness function, in our case the F1 score on the test data.
The individuals are paired using some stochastic scheme.
The best ones are selected for the following phases, while the others are discarded.

\paragraph{Reproduction}
The selected individuals have a certain probability mate with each other.
The mated pairs generate a child by randomly choosing among their genes, in this case the feature functions.
The new individual is then added to current the population.

\paragraph{Mutation}
All individuals in the population have a certain probability to mutate, i.e. change some of their genes.
Possible mutations are: gene deletion, gene insertion and gene substitution.
Multiple mutations can append at the same time.
The mutated population is used for the next epoch.

\subsubsection{Results}
In few epochs, the genetic algorithm managed to achieve the same performances of the best manually written model.
With some additional epochs of training, the algorithm managed to outperform it, reaching a F1 score of $84.15\%$.
The generated template contains many features presented in the previous sections as well as some more complex features selected by the genetic algorithm.
It is interesting to notice that some features are duplicated in many individuals generated during the search.
The removal of the duplicates causes a slightly worsening of the performances.
The \texttt{CRF++} internal optimization routine was probably not able to find the optimal weights for the feature functions, so the duplicates helped to compute better weights.

We believe that it is possible to obtain even better results by tuning the meta-parameters of the genetic algorithm and running the algorithm for some additional epochs.
This exploration is left as a future work.

Changing the regularization parameter did not improve the performances of the best model.
The genetic algorithm seems to be able to automatically pick the best features for the given regularization parameter.
