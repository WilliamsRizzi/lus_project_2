\section{Experiments}
\label{sec:experiments}

We performed the following trials:
\begin{enumerate}
    \item Train a simple model using only the words
    \item Add additional features among the original ones
    \item Create new features as a combination of the others
    \item Use a genetic algorithm to automatically create and select the best features
\end{enumerate}

\subsection{Words}
\label{subsection:words}
The first trial was to try to classify the concepts using only the words of the sentence.
This is used as the baseline for the evaluation of the performances of more complex models.
We experimented using different window sizes, i.e. creating a feature function for the current word and the preceding and following $n$ words, with $n \in [0,8]$.
In addition, we tried to enable the bigram templates of \texttt{CRF++}, i.e. considering also the label of the previous word to classify the current one. 

\begin{table}[t!]
	\centering
    \begin{tabular}{ c c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{F1, no bigram} & \multicolumn{1}{c}{F1, bigram} \\
    	\midrule
            \input{table/01_words}
    	\bottomrule
	\end{tabular}
    \caption{Performances of the models using only the words as a feature. For each window size, the model is trained with and without the bigram template. In all cases, the bigram template helps to get higher performances.}
	\label{tab:words}
\end{table}

\cref{tab:words} summaries the performances of the different models.
In all cases, the bigram template helps to get significantly higher performances.
The best model is the one with window size $2$, using bigrams.
The performances degrades for higher window sizes.
Probably the words which are very far away from the current one have very little effect on the semantic of the current one.
On the other side, the concept of the previous word seems to be very important to better classify the current one.

\subsection{POS and Stems}
\label{subsection:original}
The next step was to try with the other features initially provided, i.e. \ac{POS} and word stems.
Given the results presented in \cref{subsection:original}, we decided to consider only templates using the bigrams.

From the best template in the previous section (windows size of $2$), we started to add \ac{POS} features.
The used again the window approach, with a size $n \in [0,5]$.
The results are showed in \cref{tab:pos}.
As expected, the \ac{POS} features improved the performances of the classifier.
The best model was obtained for $n = 2$.

\begin{table}[t!]
	\centering
    \begin{tabular}{ c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{F1} \\
    	\midrule
            \input{table/02_pos}
    	\bottomrule
	\end{tabular}
    \caption{xxx.}
	\label{tab:pos}
\end{table}

Similarly as before, we repeated the same procedure for the word stems, using a window size $n \in [0,5]$.
The stem feature helped to obtain a slightly improvement, even tough not significant.
In this case, a window size of $0$ made the classifier performances slightly better, while all other attempts gave a worsening.
The results of this trial are reported in \cref{tab:stems}.

\begin{table}[t!]
	\centering
    \begin{tabular}{ c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{F1} \\
    	\midrule
            \input{table/03_stems}
    	\bottomrule
	\end{tabular}
    \caption{xxx.}
	\label{tab:stems}
\end{table}

\subsection{Additional Features}
\label{subsection:additional}
At this point, we tried to add new features.
Firstly, we played a bit with 

\subsection{Genetic Algorithm}
\label{subsection:genetic}
We decided to treat the features selection as a black-box optimization problem.
We developed a genetic algorithm using the Python \texttt{DEAP}\footnote{\url{https://github.com/DEAP/deap}} library \cite{deap} to try to select the best features function for the concept tagging task.
Following the genetic analogy \cite{lion}, we treated a \ac{CRF} template as an individual and a feature function as a gene.
A population is a set of individuals, either randomly generated or chosen among some promising templates.

The algorithm works in two phases.

\subsubsection{Initialization}
The first phase consists in the population initialization.
In our case, the population is initialized with the union of the best scoring \ac{CRF} models with some randomly generated individuals.
The random individuals are generated by generating random unigram feature functions.
To each individual, the bigram options is automatically added to the template to guarantee better performances.

\subsection{Evolution}
The second phase is the evolution, which lasts several iterations, called epochs.
During each epoch, multiple steps are performed.
After some epochs, the population should contain the best individuals found during the search.

\paragraph{Selection}
During the selection, the population evaluated using the fitness function, in our case the F1 score on the test data.
The individual are paired using some stochastic scheme.
The best ones are selected for the following phases, while the others are discarded.

\paragraph{Reproduction}
The selected individuals are mated with each other with a certain probability.
The mated pairs generate a new individual by randomly choosing among the parent's genes, in this case the feature functions.
The new individual is then added to the population.

\paragraph{Mutation}
All individuals in the population have a certain probability of mutate, i.e. change some of their genes.
Possible mutations are: gene deletion, gene insertion, gene substitution.
More mutations can append at the same time.
The mutated individuals are then used for the next epoch.

\subsubsection{Results}
In a few epochs, the genetic algorithm managed to achieve the same performances of the best manually written model.
With some additional epochs of training, the algorithm managed to significantly improve the best model so far, reaching a F1 score of $84.15\%$.
The generated template contains many features presented in the previous sections, united with more complex features build by the genetic algorithm.
It is interesting to notice that some features are duplicated.
The removal of the duplicated cause a slightly worsening of the performances.
The \texttt{CRF++} is probably not able to find the optimal weights for the feature functions and the duplicates help to obtain better weights. 

We believe that it is possible to obtain even better results by tuning the meta-parameters of the genetic algorithms and running the algorithm for additional epochs.
This exploration requires a significant amount of time and is left as a future work.
