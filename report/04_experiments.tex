\section{Experiments}
\label{sec:experiments}
We started with simple models taking into account only the words, without the additional features.
Then, we added the additional available features, one at a time.
We created quite complex models that achieved good performances.
Finally, we used a genetic algorithm to automatically select the best available features.

\subsection{Words}
\label{subsection:words}
The first trial was to try to classify the concepts using only the words of the sentence.
The result was used as the baseline for the evaluation of the performances of more complex models.
We experimented using different window sizes, i.e. creating a feature function for the current word and the preceding and following $n$ words, with $n \in [0,8]$.
In addition, we tried to enable the bigram templates of \texttt{CRF++}, i.e. considering also the label of the previous word to classify the current one. 

\begin{table}[h]
	\centering
    \begin{tabular}{ c c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{F1, no bigram} & \multicolumn{1}{c}{F1, bigram} \\
    	\midrule
            \input{table/01_words}
    	\bottomrule
	\end{tabular}
    \caption{Performances of the models using only the words. For each window size, the model is trained with and without the bigram template.}
	\label{tab:words}
\end{table}

\cref{tab:words} summaries the performances of the different models.
In all cases, the bigram template helps to get significantly higher performances.
The best model is the one with bigrams and window size $2$.
The concept of the previous word seems to be very important to better classify the current one (bigram template).
The performances improve up to window size $2$, then degrades for higher sizes.
Probably the words very distant from the current one have little effect on the semantic of the current one.
Taking them into account makes the model overfit the training data.

\subsection{POS}
\label{subsection:original}
The next step was to try with the other features initially provided, i.e. \ac{POS} and word stems.
Given the results presented in \cref{subsection:original}, we decided to consider only templates using the bigrams.

From the best template in the previous section (windows size of $2$), we started to add \ac{POS} features.
The used again the window approach, with a size $n \in [0,5]$.
The results are showed in \cref{tab:pos}.
As expected, the \ac{POS} features improved the performances of the classifier.
The best model was obtained for $n = 2$.
Similarly to the previous models, a large window seems to overfit the training data.

\begin{table}[h]
	\centering
    \begin{tabular}{ c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{F1} \\
    	\midrule
            \input{table/02_pos}
    	\bottomrule
	\end{tabular}
    \caption{Performances of the models using words and POS tags for different window sizes.}
	\label{tab:pos}
\end{table}

\subsection{Stems}
We repeated the same procedure for the word stems, using a window size $n \in [0,5]$.
The stem feature helped to obtain a slightly improvement, even tough not significant.
In this case, a window size of $0$ made the classifier performances slightly better, while all other attempts gave a worsening.
The results of this trial are reported in \cref{tab:stems}.

\begin{table}[h]
	\centering
    \begin{tabular}{ c c }
    	\toprule
    		\multicolumn{1}{c}{n} & \multicolumn{1}{c}{F1} \\
    	\midrule
            \input{table/03_stems}
    	\bottomrule
	\end{tabular}
    \caption{Performances of the models using words, POS tags and stems for different window sizes.}
	\label{tab:stems}
\end{table}

\subsection{Additional Features}
\label{subsection:additional}
At this point, we tried to add new features. 
Since many concepts spans on multiple words, we started to add features for sequences of successive 2 words, \ac{POS} tags and stems.
Then, we tried to combine words and \ac{POS} tags of closed words.

\subsection{Genetic Algorithm}
\label{subsection:genetic}
In order to select the best feature functions for the concept tagging task, we decided to the features selection as a black-box optimization problem.
We developed a genetic algorithm using the Python \texttt{DEAP}\footnote{\url{https://github.com/DEAP/deap}} library \cite{deap}.
Following the genetic analogy \cite{lion}, we treated a \ac{CRF} template as an individual and a feature function as a gene.
A set of individuals form a population.

\subsubsection{Initialization}
The algorithm is divided two phases.
The first one consists in the population initialization.
In our case, the initial population is composed by the union of the best scoring \ac{CRF} models with some randomly generated individuals.
To generate a new individual, it is sufficient to randomly select a set of genes.
The bigram options is automatically added to the entire population to guarantee better performances.

\subsection{Evolution}
The second phase is the evolution.
The evolution lasts several epochs.
Each epoch generate a new population starting from the current one, trying to select the best individuals.
After the last iteration, the population should contain the best individuals found during the search.
At each epoch, the following steps are performed.

\paragraph{Selection}
The population evaluated using the fitness function, in our case the F1 score on the test data.
The individual are paired using some stochastic scheme.
The best ones are selected for the following phases, while the others are discarded.

\paragraph{Reproduction}
The selected individuals have a certain probability to be mated with each other.
The mated pairs generate a children by randomly choosing among their genes, in this case the feature functions.
The new individual is then added to current the population.

\paragraph{Mutation}
All individuals in the population have a certain probability of mutate, i.e. change some of their genes.
Possible mutations are: gene deletion, gene insertion and gene substitution.
More mutations can append at the same time.
The mutated population is used for the next epoch.

\subsubsection{Results}
In a few epochs, the genetic algorithm managed to achieve the same performances of the best manually written model.
With some additional epochs of training, the algorithm managed to significantly improve the best model so far, reaching a F1 score of $84.15\%$.
The generated template contains many features presented in the previous sections and some more complex features chosen by the genetic algorithm.
It is interesting to notice that some features are duplicated in most individuals generated during the search.
The removal of the duplicated cause a slightly worsening of the performances.
The \texttt{CRF++} was probably not able to find the optimal weights for the feature functions and the duplicates help to obtain better weights.

We believe that it is possible to obtain even better results by tuning the meta-parameters of the genetic algorithm and running the algorithm for additional epochs.
This exploration requires a significant amount of time and is left as a future work.
